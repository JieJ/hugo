	<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.14" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> neural network--backpropagation &middot; My New Hugo Site </title>

  
  <link rel="stylesheet" href="http://localhost:1313/css/poole.css">
  <link rel="stylesheet" href="http://localhost:1313/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:1313/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="My New Hugo Site" />
  
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$']]},
        "HTML-CSS": 
          {scale: 92},
        TeX: { equationNumbers: { autoNumber: "AMS" }}});
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</script>
</head>

	<body class="">
		<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://localhost:1313/"><h1>My New Hugo Site</h1></a>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
    </ul>

    <p>&copy; 2015. All rights reserved. </p>
  </div>
</div>


		<div class="content container">
			<div class="post">
			 	<h1>neural network--backpropagation</h1>
			  <span class="post-date">Tue, Sep 8, 2015</span>
			      

<h4 id="how-the-backpropagation-algorithm-works:dc0e0e4f946020fd37f983facbed50cf">How the backpropagation algorithm works</h4>

<h6 id="1-warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network:dc0e0e4f946020fd37f983facbed50cf">1. Warm up: a fast matrix-based approach to computing the output from a neural network</h6>

<p>基于矩阵前向计算神经网络输出。</p>

<h6 id="2-the-two-assumptions-we-need-about-the-cost-function:dc0e0e4f946020fd37f983facbed50cf">2. The two assumptions we need about the cost function</h6>

<ul>
<li>cost function can be written as an average
$$ C = \frac {1}{n} \sum_xC_x $$</li>
<li>cost function can be written as a function of the outputs from the neural network
$\begin{eqnarray} C = \frac{1}{2} |y-a^L|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2,\tag{27}\end{eqnarray}$
对于输出层来说，我们就是这么做的。
但是对于隐藏层，如何将损失函数写成是关于这一层输出的函数呢？</li>
</ul>

<h5 id="3-the-hadamard-product-s-t:dc0e0e4f946020fd37f983facbed50cf">3. The Hadamard product, s⊙t</h5>

<p>elementwise multiplication
$\begin{eqnarray}
\left[\begin{array}{c} 1 \ 2 \end{array}\right]
  \odot \left[\begin{array}{c} 3 \ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 \ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 \ 8 \end{array} \right].
\tag{28}\end{eqnarray}$</p>

<h5 id="4-the-four-fundamental-equations-behind-backpropagation:dc0e0e4f946020fd37f983facbed50cf">4. The four fundamental equations behind backpropagation</h5>

<p>首先计算神经元残差 $\delta^l<em>j$
再将残差与梯度 $\partial C/ \partial w^l</em>{jk}$ 与 $\partial C/ \partial b^l<em>j$ 联系起来。
$\partial C/ \partial w^l</em>{jk}$</p>

			</div>

			
		</div>

  <script data-no-instant>document.write('<script src="http://'
        + (location.host || 'localhost').split(':')[0]
		+ ':1313/livereload.js?mindelay=10"></'
        + 'script>')</script></body>
</html>
