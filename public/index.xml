<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My New Hugo Site</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ch-en</language>
    <lastBuildDate>Tue, 08 Sep 2015 14:23:15 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>neural network--backpropagation</title>
      <link>http://localhost:1313/post/neural%20network-3/</link>
      <pubDate>Tue, 08 Sep 2015 14:23:15 +0800</pubDate>
      
      <guid>http://localhost:1313/post/neural%20network-3/</guid>
      <description>

&lt;h4 id=&#34;how-the-backpropagation-algorithm-works:dc0e0e4f946020fd37f983facbed50cf&#34;&gt;How the backpropagation algorithm works&lt;/h4&gt;

&lt;h6 id=&#34;1-warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network:dc0e0e4f946020fd37f983facbed50cf&#34;&gt;1. Warm up: a fast matrix-based approach to computing the output from a neural network&lt;/h6&gt;

&lt;p&gt;基于矩阵前向计算神经网络输出。&lt;/p&gt;

&lt;h6 id=&#34;2-the-two-assumptions-we-need-about-the-cost-function:dc0e0e4f946020fd37f983facbed50cf&#34;&gt;2. The two assumptions we need about the cost function&lt;/h6&gt;

&lt;ul&gt;
&lt;li&gt;cost function can be written as an average
$$ C = \frac {1}{n} \sum_xC_x $$&lt;/li&gt;
&lt;li&gt;cost function can be written as a function of the outputs from the neural network
$\begin{eqnarray} C = \frac{1}{2} |y-a^L|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2,\tag{27}\end{eqnarray}$
对于输出层来说，我们就是这么做的。
但是对于隐藏层，如何将损失函数写成是关于这一层输出的函数呢？&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;3-the-hadamard-product-s-t:dc0e0e4f946020fd37f983facbed50cf&#34;&gt;3. The Hadamard product, s⊙t&lt;/h5&gt;

&lt;p&gt;elementwise multiplication
$\begin{eqnarray}
\left[\begin{array}{c} 1 \ 2 \end{array}\right]
  \odot \left[\begin{array}{c} 3 \ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 \ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 \ 8 \end{array} \right].
\tag{28}\end{eqnarray}$&lt;/p&gt;

&lt;h5 id=&#34;4-the-four-fundamental-equations-behind-backpropagation:dc0e0e4f946020fd37f983facbed50cf&#34;&gt;4. The four fundamental equations behind backpropagation&lt;/h5&gt;

&lt;p&gt;首先计算神经元残差 $\delta^l&lt;em&gt;j$
再将残差与梯度 $\partial C/ \partial w^l&lt;/em&gt;{jk}$ 与 $\partial C/ \partial b^l&lt;em&gt;j$ 联系起来。
$\partial C/ \partial w^l&lt;/em&gt;{jk}$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>